{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Quantum Circuit Born Machine\n",
    "\n",
    "Quantum circuit born machine is a fresh approach to quantum machine learning. It use a parameterized quantum circuit to learning machine learning tasks with gradient based optimization. In this tutorial, we will show how to implement it with **Yao** (幺) framework.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "using Yao, GR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "about the framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "?幺"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training target\n",
    "a gaussian distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function gaussian_pdf(n, μ, σ)\n",
    "    x = collect(1:1<<n)\n",
    "    pl = @. 1 / sqrt(2pi * σ^2) * exp(-(x - μ)^2 / (2 * σ^2))\n",
    "    pl / sum(pl)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "f(x \\left| \\mu, \\sigma^2\\right) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "const n = 6\n",
    "const maxiter = 20\n",
    "pg = gaussian_pdf(n, 2^5-0.5, 2^4)\n",
    "fig = plot(0:1<<n-1, pg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Circuits\n",
    "\n",
    "## Building Blocks\n",
    "\n",
    "Gates are grouped to become a layer in a circuit, this layer can be **Arbitrary Rotation** or **CNOT entangler**. Which are used as our basic building blocks of **Born Machines**.\n",
    "\n",
    "### Arbitrary Rotation\n",
    "\n",
    "Arbitrary Rotation is built with **Rotation Gate on Z**, **Rotation Gate on X** and **Rotation Gate on Z**:\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "Rz(\\theta) \\cdot Rx(\\theta) \\cdot Rz(\\theta)\n",
    "\\end{equation}\n",
    "\n",
    "Since our input will be a $|0\\dots 0\\rangle$ state. The first layer of arbitrary rotation can just use $Rx(\\theta) \\cdot Rz(\\theta)$ and the last layer of arbitrary rotation could just use $Rz(\\theta)\\cdot Rx(\\theta)$\n",
    "\n",
    "\n",
    "In **幺**, every Hilbert operator is a **block** type, this includes all **quantum gates** and **quantum oracles**. In general, operators appears in a quantum circuit can be divided into **Composite Blocks** and **Primitive Blocks**.\n",
    "\n",
    "We follow the low abstraction principle and thus each block represents a certain approach of calculation. The simplest **Composite Block** is a **Chain Block**, which chains other blocks (oracles) with the same number of qubits together. It is just a simple mathematical composition of operators with same size. e.g.\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{chain(X, Y, Z)} \\iff X \\cdot Y \\cdot Z\n",
    "\\end{equation}\n",
    "\n",
    "We can construct an arbitrary rotation block by chain $Rz$, $Rx$, $Rz$ together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain(Rz(0), Rx(0), Rz(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Rx`, `Ry` and `Rz` will construct new rotation gate, which are just shorthands for `rot(X, 0.0)`, etc.\n",
    "\n",
    "Then, let's pile them up vertically with another method called **rollrepeat**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer(x::Symbol) = layer(Val(x))\n",
    "layer(::Val{:first}) = rollrepeat(chain(Rx(0), Rz(0)))\n",
    "rollrepeat(chain(Rx(0), Rz(0)))(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In **幺**, the factory method **rollrepeat** will construct a block called **Roller**. It is mathematically equivalent to the kronecker product of all operators in this layer:\n",
    "\n",
    "\\begin{equation}\n",
    "rollrepeat(n, U) \\iff roll(n, \\text{i=>U for i = 1:n}) \\iff kron(n, \\text{i=>U for i=1:n}) \\iff U \\otimes \\dots \\otimes U\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?Val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roll(4, i=>X for i = 1:4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rollrepeat(4, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kron(4, i=>X for i = 1:4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, `kron` is calculated differently comparing to `roll`. In principal, **Roller** will be able to calculate small blocks with same size with higher efficiency. But for large blocks **Roller** may be slower. In **幺**, we offer you this freedom to choose the most suitable solution.\n",
    "\n",
    "\n",
    "all factory methods will **lazy** evaluate **the first arguements**, which is the number of qubits. It will return a lambda function that requires a single interger input. The instance of desired block will only be constructed until all the information is filled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rollrepeat(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rollrepeat(X)(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you filled all the information in somewhere of the declaration, 幺 will be able to infer the others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain(4, rollrepeat(X), rollrepeat(Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now define the rest of rotation layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer(::Val{:last}) = rollrepeat(chain(Rz(0), Rx(0)))\n",
    "layer(::Val{:mid}) = rollrepeat(chain(Rz(0), Rx(0), Rz(0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNOT Entangler\n",
    "\n",
    "Another component of quantum circuit born machine is several **CNOT** operators applied on different qubits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entangler(pairs) = chain(control([ctrl, ], target=>X) for (ctrl, target) in pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then define such a born machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function QCBM(n, nlayer, pairs)\n",
    "    circuit = chain(n)\n",
    "    push!(circuit, layer(:first))\n",
    "    \n",
    "    for i = 1:(nlayer - 1)\n",
    "        push!(circuit, cache(entangler(pairs)))\n",
    "        push!(circuit, layer(:mid))\n",
    "    end\n",
    "    \n",
    "    push!(circuit, cache(entangler(pairs)))\n",
    "    push!(circuit, layer(:last))\n",
    "    \n",
    "    circuit\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the method `cache` here to tag the entangler block that it should be cached after its first run, because it is actually a constant oracle. Let's see what will be constructed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QCBM(4, 1, [1=>2, 2=>3, 3=>4, 4=>1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MMD Loss & Gradients\n",
    "\n",
    "The MMD loss is describe below:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\mathcal{L} &= \\left| \\sum_{x} p \\theta(x) \\phi(x) - \\sum_{x} \\pi(x) \\phi(x) \\right|^2\\\\\n",
    "            &= \\langle K(x, y) \\rangle_{x \\sim p_{\\theta}, y\\sim p_{\\theta}} - 2 \\langle K(x, y) \\rangle_{x\\sim p_{\\theta}, y\\sim \\pi} + \\langle K(x, y) \\rangle_{x\\sim\\pi, y\\sim\\pi}\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "We will use a squared exponential kernel here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct Kernel\n",
    "    sigma::Float64\n",
    "    matrix::Matrix{Float64}    \n",
    "end\n",
    "\n",
    "function Kernel(nqubits, sigma)\n",
    "    basis = collect(0:(1<<nqubits - 1))\n",
    "    Kernel(sigma, kernel_matrix(basis, basis, sigma))\n",
    "end\n",
    "\n",
    "expect(kernel::Kernel, px::Vector{Float64}, py::Vector{Float64}) = px' * kernel.matrix * py\n",
    "loss(qcbm, kernel::Kernel, ptrain) = (p = get_prob(qcbm) - ptrain; expect(kernel, p, p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's define the kernel matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function kernel_matrix(x, y, sigma)\n",
    "    dx2 = (x .- y').^2\n",
    "    gamma = 1.0 / (2 * sigma)\n",
    "    K = exp.(-gamma * dx2)\n",
    "    K\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradients\n",
    "\n",
    "\n",
    "the gradient of MMD loss is\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\theta^i_l} &= \\langle K(x, y) \\rangle_{x\\sim p_{\\theta^+}, y\\sim p_{\\theta}} - \\langle K(x, y) \\rangle_{x\\sim p_{\\theta}^-, y\\sim p_{\\theta}}\\\\\n",
    "&- \\langle K(x, y) \\rangle _{x\\sim p_{\\theta^+}, y\\sim\\pi} + \\langle K(x, y) \\rangle_{x\\sim p_{\\theta^-}, y\\sim\\pi}\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "We have to update one parameter of each rotation gate each time, and calculate its gradient then collect them. Since we will need to calculate the probability from the state vector frequently, let's define a shorthand first.\n",
    "\n",
    "\n",
    "Firstly, you have to define a quantum register. Each run of a QCBM's input is a simple $|00\\cdots 0\\rangle$ state. We provide string literal `bit` to help you define one-hot state vectors like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = register(bit\"0000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "circuit = QCBM(6, 10, [1=>2, 3=>4, 5=>6, 2=>3, 4=>5, 6=>1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we define its shorthand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_prob(qcbm) = apply!(register(bit\"0\"^nqubits(qcbm)), qcbm) |> statevec .|> abs2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first iterate through each layer contains rotation gates and allocate an array to store our gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function gradient(n, nlayers, qcbm, kernel, ptrain)\n",
    "    prob = get_prob(qcbm)\n",
    "    grad = zeros(real(datatype(qcbm)), nparameters(qcbm))\n",
    "    idx = 0\n",
    "    for ilayer = 1:2:(2 * nlayers + 1)\n",
    "        idx = grad_layer!(grad, idx, prob, qcbm, qcbm[ilayer], kernel, ptrain)\n",
    "    end\n",
    "    grad\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we iterate through each rotation gate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function grad_layer!(grad, idx, prob, qcbm, layer, kernel, ptrain)\n",
    "    count = idx\n",
    "    for each_line in blocks(layer)\n",
    "        for each in blocks(each_line)\n",
    "            gradient!(grad, count+1, prob, qcbm, each, kernel, ptrain)\n",
    "            count += 1\n",
    "        end\n",
    "    end\n",
    "    count\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We update each parameter by rotate it $-\\pi/2$ and $\\pi/2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function gradient!(grad, idx, prob, qcbm, gate, kernel, ptrain)\n",
    "    dispatch!(+, gate, pi / 2)\n",
    "    prob_pos = get_prob(qcbm)\n",
    "\n",
    "    dispatch!(-, gate, pi)\n",
    "    prob_neg = get_prob(qcbm)\n",
    "\n",
    "    dispatch!(+, gate, pi / 2) # set back\n",
    "\n",
    "    grad_pos = expect(kernel, prob, prob_pos) - expect(kernel, prob, prob_neg)\n",
    "    grad_neg = expect(kernel, ptrain, prob_pos) - expect(kernel, ptrain, prob_neg)\n",
    "    grad[idx] = grad_pos - grad_neg\n",
    "    grad\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer\n",
    "\n",
    "We will use the Adam optimizer. Since we don't want you to install another package for this, the following code for this optimizer is copied from [Knet.jl](https://github.com/denizyuret/Knet.jl)\n",
    "\n",
    "Reference: [Kingma, D. P., & Ba,\n",
    "J. L. (2015)](https://arxiv.org/abs/1412.6980). Adam: a Method for\n",
    "Stochastic Optimization. International Conference on Learning\n",
    "Representations, 1–13."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using LinearAlgebra\n",
    "\n",
    "mutable struct Adam\n",
    "    lr::AbstractFloat\n",
    "    gclip::AbstractFloat\n",
    "    beta1::AbstractFloat\n",
    "    beta2::AbstractFloat\n",
    "    eps::AbstractFloat\n",
    "    t::Int\n",
    "    fstm\n",
    "    scndm\n",
    "end\n",
    "\n",
    "Adam(; lr=0.001, gclip=0, beta1=0.9, beta2=0.999, eps=1e-8)=Adam(lr, gclip, beta1, beta2, eps, 0, nothing, nothing)\n",
    "\n",
    "function update!(w, g, p::Adam)\n",
    "    gclip!(g, p.gclip)\n",
    "    if p.fstm===nothing; p.fstm=zero(w); p.scndm=zero(w); end\n",
    "    p.t += 1\n",
    "    lmul!(p.beta1, p.fstm)\n",
    "    BLAS.axpy!(1-p.beta1, g, p.fstm)\n",
    "    lmul!(p.beta2, p.scndm)\n",
    "    BLAS.axpy!(1-p.beta2, g .* g, p.scndm)\n",
    "    fstm_corrected = p.fstm / (1 - p.beta1 ^ p.t)\n",
    "    scndm_corrected = p.scndm / (1 - p.beta2 ^ p.t)\n",
    "    BLAS.axpy!(-p.lr, @.(fstm_corrected / (sqrt(scndm_corrected) + p.eps)), w)\n",
    "end\n",
    "\n",
    "function gclip!(g, gclip)\n",
    "    if gclip == 0\n",
    "        g\n",
    "    else\n",
    "        gnorm = vecnorm(g)\n",
    "        if gnorm <= gclip\n",
    "            g\n",
    "        else\n",
    "            BLAS.scale!(gclip/gnorm, g)\n",
    "        end\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training of the quantum circuit is simple, just iterate through the steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function train!(qcbm, ptrain, optim; learning_rate=0.1, niter=50)\n",
    "    # initialize the parameters\n",
    "    params = 2pi * rand(nparameters(qcbm))\n",
    "    dispatch!(qcbm, params)\n",
    "    kernel = Kernel(nqubits(qcbm), 0.25)\n",
    "    \n",
    "    n, nlayers = nqubits(qcbm), (length(qcbm)-1)÷2\n",
    "    history = Float64[]\n",
    "    \n",
    "    for i = 1:niter\n",
    "        grad = gradient(n, nlayers, qcbm, kernel, ptrain)\n",
    "        curr_loss = loss(qcbm, kernel, ptrain)\n",
    "        push!(history, curr_loss)        \n",
    "        params = collect(parameters(qcbm))\n",
    "        update!(params, grad, optim)\n",
    "        dispatch!(qcbm, params)\n",
    "    end\n",
    "    history\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "optim = Adam(lr=0.1)\n",
    "his = train!(circuit, pg, optim, niter=50, learning_rate=0.1)\n",
    "plot(1:50, his, xlabel=\"iteration\", ylabel=\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = get_prob(circuit)\n",
    "plot(0:1<<n-1, p, pg, xlabel=\"x\", ylabel=\"p\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.0.0",
   "language": "julia",
   "name": "julia-1.0"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 3
}
